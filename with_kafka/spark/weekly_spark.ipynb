{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "spark = SparkSession.builder.appName('weekly_spark').getOrCreate()\n",
    "#spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType\n",
    "from pyspark.sql.functions import length, col, count, expr, monotonically_increasing_id, lit\n",
    "\n",
    "reviews_schema = StructType([\n",
    "    StructField(\"App ID\", IntegerType(), True),\n",
    "    StructField(\"Review\", StringType(), True),\n",
    "    StructField(\"Voted Up\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "top_sellers_schema = StructType([\n",
    "    StructField(\"Rank\", IntegerType(), True),\n",
    "    StructField(\"Game Name\", StringType(), True),\n",
    "    StructField(\"Free to Play\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "top_sellers_appids_schema = StructType([\n",
    "    StructField(\"App ID\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEEKLY_DATA_PATH = r'../data/weekly_data/'\n",
    "reviews_path = os.path.join(WEEKLY_DATA_PATH, 'reviews/')\n",
    "FILE_DATE = None\n",
    "\n",
    "try:\n",
    "    csv_files = [f for f in os.listdir(reviews_path) if f.endswith('.csv')]\n",
    "    if csv_files:\n",
    "        csv_file = csv_files[0]\n",
    "        FILE_DATE = csv_file[0].split('.')[0].split('_')[0]\n",
    "        csv_file_path = os.path.join(reviews_path, csv_file)\n",
    "        most_daily_played = spark.read.csv(csv_file_path, header=True, schema=reviews_schema)\n",
    "    else:\n",
    "        print(\"No CSV files found in the 'reviews_path' directory.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while reading the CSV file:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "most_daily_played = most_daily_played.na.drop(subset=[\"Review\", \"Voted Up\", \"App ID\"])\n",
    "most_daily_played = most_daily_played.filter(length(col(\"Review\")) >= 2)\n",
    "\n",
    "# Counting the number of positive and negative reviews\n",
    "counted_reviews = most_daily_played.groupBy(\"App ID\").pivot(\"Voted Up\", [\"pos\", \"neg\"]).agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Seprarating the positive and negative reviews\n",
    "neg_reviews_df = most_daily_played.filter(most_daily_played[\"Voted Up\"] == \"neg\")\n",
    "pos_reviews_df = most_daily_played.filter(most_daily_played[\"Voted Up\"] == \"pos\")\n",
    "\n",
    "neg_reviews_df = neg_reviews_df.withColumn(\"FILE_DATE\", lit(FILE_DATE))\n",
    "pos_reviews_df = pos_reviews_df.withColumn(\"FILE_DATE\", lit(FILE_DATE))\n",
    "counted_reviews = counted_reviews.withColumn(\"FILE_DATE\", lit(FILE_DATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top_sellers\n",
    "WEEKLY_TOP_SELLERS_PATH = WEEKLY_DATA_PATH + r'top_sellers/'\n",
    "files = os.listdir(WEEKLY_TOP_SELLERS_PATH)\n",
    "\n",
    "FILE_DATE = None\n",
    "try:\n",
    "    csv_file1 = [f for f in files if f.endswith('weekly_top_sellers.csv')]\n",
    "    FILE_DATE = csv_file1[0].split('.')[0].split('_')[0]\n",
    "    top_sellers_games = spark.read.csv(\n",
    "        WEEKLY_TOP_SELLERS_PATH + csv_file1[0],\n",
    "        header=True,\n",
    "        schema=top_sellers_schema  \n",
    "    )\n",
    "    csv_file2 = [f for f in files if f.endswith('weekly_top_sellers_appIds.csv')]\n",
    "    FILE_DATE = csv_file2[0].split('.')[0].split('_')[0]\n",
    "    top_sellers_appids = spark.read.csv(\n",
    "        WEEKLY_TOP_SELLERS_PATH + csv_file2[0],\n",
    "        header=True,\n",
    "        schema=top_sellers_appids_schema  \n",
    "    )\n",
    "    top_sellers_appids = top_sellers_appids.withColumn(\n",
    "    \"Rank\",(monotonically_increasing_id() + 1).cast(\"int\"))\n",
    "    top_sellers = top_sellers_games.join(top_sellers_appids, on=[\"Rank\"], how=\"inner\")\n",
    "    top_sellers = top_sellers.withColumn(\"FILE_DATE\", lit(FILE_DATE))\n",
    "    #top_sellers.show()   \n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while reading the CSV file:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while reading the JSON file: 'charmap' codec can't decode byte 0x81 in position 15334: character maps to <undefined>\n"
     ]
    }
   ],
   "source": [
    "#news\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "WEEKLY_NEWS_PATH = WEEKLY_DATA_PATH + r'news/'\n",
    "files = os.listdir(WEEKLY_NEWS_PATH)\n",
    "dfs = []\n",
    "\n",
    "try:\n",
    "    for file in files:\n",
    "        with open(WEEKLY_NEWS_PATH + file, 'r') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "\n",
    "            df = pd.DataFrame() \n",
    "            for i in range(len(json_data['appnews']['newsitems'])):\n",
    "                column_name = f\"contents_{i}\" \n",
    "                value = json_data['appnews']['newsitems'][i]['contents']\n",
    "                if isinstance(value, (list, dict)):\n",
    "                    value = str(value)\n",
    "                df[column_name] = [value]\n",
    "            df['App ID'] = json_data['appnews']['appid']\n",
    "            dfs.append(df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while reading the JSON file:\", e)\n",
    "    \n",
    "news_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "columns_to_iterate = news_df.columns[:-1]\n",
    "for column in columns_to_iterate:\n",
    "    for index, cell in enumerate(news_df[column]):\n",
    "        if cell is not None:\n",
    "            cleaned_text = re.sub(r'<strong>|</strong>|<a>|</a>|<a\\s+href\\s*=\\s*\".*?\"\\s*>\\s*|https://.*|\\{STEAM_CLAN_IMAGE\\}.*', '', cell)\n",
    "            news_df.at[index, column] = cleaned_text\n",
    "\n",
    "news_df.to_csv(r'../data/weekly_data/news/spark_modified_news.csv', index=False, sep='\\t')\n",
    "news_path = os.path.join(WEEKLY_DATA_PATH, 'news/')\n",
    "\n",
    "try:\n",
    "    csv_files = [f for f in os.listdir(news_path) if f.endswith('.csv')]\n",
    "    if csv_files:\n",
    "        csv_file = csv_files[0]\n",
    "        FILE_DATE = csv_file.split('.')[0].split('_')[0]  \n",
    "        csv_file_path = os.path.join(news_path, csv_file)\n",
    "        news_spark_df = spark.read.option(\"delimiter\", \"\\t\").csv(csv_file_path, header=True)\n",
    "    else:\n",
    "        print(\"No CSV files found in the 'news_path' directory.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while reading the CSV file:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Local Disk D\\BigDataProject\\with_kafka\\spark\\weekly_spark.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/BigDataProject/with_kafka/spark/weekly_spark.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m counted_reviews\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmode(\u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msave(counted_reviews_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/BigDataProject/with_kafka/spark/weekly_spark.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m top_sellers\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmode(\u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msave(top_sellers_path)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/BigDataProject/with_kafka/spark/weekly_spark.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m news_spark_df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmode(\u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msave(news_spark_df)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "neg_reviews_path = r\"../saved_data/weekly_data/neg_reviews\"\n",
    "pos_reviews_path = r\"../saved_data/weekly_data/pos_reviews\"\n",
    "counted_reviews_path = r\"../saved_data/weekly_data/counted_reviews\"\n",
    "top_sellers_path = r\"../saved_data/weekly_data/top_sellers\"\n",
    "news_spark_path = r\"../saved_data/weekly_data/news_spark_df\"\n",
    "\n",
    "# Save the DataFrame as CSV\n",
    "neg_reviews_df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(neg_reviews_path)\n",
    "pos_reviews_df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(pos_reviews_path)\n",
    "counted_reviews.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(counted_reviews_path)\n",
    "top_sellers.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(top_sellers_path)\n",
    "news_spark_df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(news_spark_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
