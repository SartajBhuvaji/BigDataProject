{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "spark = SparkSession.builder.appName('weekly_spark').getOrCreate()\n",
    "#spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType\n",
    "from pyspark.sql.functions import length, col, count, expr, monotonically_increasing_id, lit\n",
    "\n",
    "reviews_schema = StructType([\n",
    "    StructField(\"App ID\", IntegerType(), True),\n",
    "    StructField(\"Review\", StringType(), True),\n",
    "    StructField(\"Voted Up\", StringType(), True)\n",
    "])\n",
    "\n",
    "weekly_top_news_schema = StructType([\n",
    "    StructField(\"appnews\", StructType([\n",
    "        StructField(\"appid\", IntegerType()),\n",
    "        StructField(\"newsitems\", ArrayType(StructType([\n",
    "            StructField(\"gid\", StringType()),\n",
    "            StructField(\"title\", StringType()),\n",
    "            StructField(\"url\", StringType()),\n",
    "            StructField(\"is_external_url\", BooleanType()),\n",
    "            StructField(\"author\", StringType()),\n",
    "            StructField(\"contents\", StringType()),\n",
    "            StructField(\"feedlabel\", StringType()),\n",
    "            StructField(\"date\", IntegerType()),\n",
    "            StructField(\"feedname\", StringType()),\n",
    "            StructField(\"feed_type\", IntegerType()),\n",
    "            StructField(\"appid\", IntegerType()),\n",
    "            StructField(\"tags\", ArrayType(StringType()))\n",
    "        ]))),\n",
    "        StructField(\"count\", IntegerType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "top_sellers_schema = StructType([\n",
    "    StructField(\"Rank\", IntegerType(), True),\n",
    "    StructField(\"Game Name\", StringType(), True),\n",
    "    StructField(\"Free to Play\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "top_sellers_appids_schema = StructType([\n",
    "    StructField(\"App ID\", IntegerType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEEKLY_DATA_PATH = r'../data/weekly_data/'\n",
    "reviews_path = os.path.join(WEEKLY_DATA_PATH, 'reviews/')\n",
    "FILE_DATE = None\n",
    "\n",
    "try:\n",
    "    csv_files = [f for f in os.listdir(reviews_path) if f.endswith('.csv')]\n",
    "    if csv_files:\n",
    "        csv_file = csv_files[0]\n",
    "        FILE_DATE = csv_file[0].split('.')[0].split('_')[0]\n",
    "        csv_file_path = os.path.join(reviews_path, csv_file)\n",
    "        most_daily_played = spark.read.csv(csv_file_path, header=True, schema=reviews_schema)\n",
    "    else:\n",
    "        print(\"No CSV files found in the 'reviews_path' directory.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while reading the CSV file:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "most_daily_played = most_daily_played.na.drop(subset=[\"Review\", \"Voted Up\", \"App ID\"])\n",
    "most_daily_played = most_daily_played.filter(length(col(\"Review\")) >= 2)\n",
    "\n",
    "# Counting the number of positive and negative reviews\n",
    "counted_reviews = most_daily_played.groupBy(\"App ID\").pivot(\"Voted Up\", [\"pos\", \"neg\"]).agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Seprarating the positive and negative reviews\n",
    "neg_reviews_df = most_daily_played.filter(most_daily_played[\"Voted Up\"] == \"neg\")\n",
    "pos_reviews_df = most_daily_played.filter(most_daily_played[\"Voted Up\"] == \"pos\")\n",
    "\n",
    "neg_reviews_df = neg_reviews_df.withColumn(\"FILE_DATE\", lit(FILE_DATE))\n",
    "pos_reviews_df = pos_reviews_df.withColumn(\"FILE_DATE\", lit(FILE_DATE))\n",
    "counted_reviews = counted_reviews.withColumn(\"FILE_DATE\", lit(FILE_DATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top_sellers\n",
    "WEEKLY_TOP_SELLERS_PATH = WEEKLY_DATA_PATH + r'top_sellers/'\n",
    "files = os.listdir(WEEKLY_TOP_SELLERS_PATH)\n",
    "\n",
    "FILE_DATE = None\n",
    "try:\n",
    "    csv_file1 = [f for f in files if f.endswith('weekly_top_sellers.csv')]\n",
    "    FILE_DATE = csv_file1[0].split('.')[0].split('_')[0]\n",
    "    top_sellers_games = spark.read.csv(\n",
    "        WEEKLY_TOP_SELLERS_PATH + csv_file1[0],\n",
    "        header=True,\n",
    "        schema=top_sellers_schema  \n",
    "    )\n",
    "    csv_file2 = [f for f in files if f.endswith('weekly_top_sellers_appIds.csv')]\n",
    "    FILE_DATE = csv_file2[0].split('.')[0].split('_')[0]\n",
    "    top_sellers_appids = spark.read.csv(\n",
    "        WEEKLY_TOP_SELLERS_PATH + csv_file2[0],\n",
    "        header=True,\n",
    "        schema=top_sellers_appids_schema  \n",
    "    )\n",
    "    top_sellers_appids = top_sellers_appids.withColumn(\n",
    "    \"Rank\",(monotonically_increasing_id() + 1).cast(\"int\"))\n",
    "    top_sellers = top_sellers_games.join(top_sellers_appids, on=[\"Rank\"], how=\"inner\")\n",
    "    top_sellers = top_sellers.withColumn(\"FILE_DATE\", lit(FILE_DATE))\n",
    "    #top_sellers.show()   \n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while reading the CSV file:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Local Disk D\\BigDataProject\\with_kafka\\spark\\weekly_spark.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/BigDataProject/with_kafka/spark/weekly_spark.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pos_reviews_df\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmode(\u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msave(pos_reviews_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/BigDataProject/with_kafka/spark/weekly_spark.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m counted_reviews\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmode(\u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msave(counted_reviews_path)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Local%20Disk%20D/BigDataProject/with_kafka/spark/weekly_spark.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m top_sellers_path\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmode(\u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msave(top_sellers_path)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "neg_reviews_path = r\"../saved_data/weekly_data/neg_reviews\"\n",
    "pos_reviews_path = r\"../saved_data/weekly_data/pos_reviews\"\n",
    "counted_reviews_path = r\"../saved_data/weekly_data/counted_reviews\"\n",
    "top_sellers_path = r\"../saved_data/weekly_data/top_sellers\"\n",
    "\n",
    "# Save the DataFrame as CSV\n",
    "neg_reviews_df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(neg_reviews_path)\n",
    "pos_reviews_df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(pos_reviews_path)\n",
    "counted_reviews.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(counted_reviews_path)\n",
    "top_sellers.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(top_sellers_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to work on news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = None\n",
    "WEEKLY_TOP_NEWS_PATH = WEEKLY_DATA_PATH + r'news/'\n",
    "files = os.listdir(WEEKLY_TOP_NEWS_PATH)\n",
    "try:\n",
    "    json_files = [pos_json for pos_json in files if pos_json.endswith('.json')]\n",
    "    for file in json_files:\n",
    "        steam_game_news = spark.read.json(\n",
    "            WEEKLY_TOP_NEWS_PATH + file,\n",
    "            multiLine=True,\n",
    "            schema = weekly_top_news_schema     \n",
    "        )\n",
    "        if merged_df is None:\n",
    "            merged_df = steam_game_news\n",
    "        else:\n",
    "            merged_df = merged_df.union(steam_game_news)\n",
    "except:\n",
    "    print('No json files found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = WEEKLY_TOP_NEWS_PATH + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(temp):\n",
    "    print(f\"The JSON file at {temp} exists.\")\n",
    "else:\n",
    "    print(f\"The JSON file at {temp} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "# Extract the 'appid' value as a string from the 'appnews' column\n",
    "split_df = merged_df.withColumn(\"appid\", expr(\"cast(appnews.appid as string)\"))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "split_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_game_news.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews\n",
    "WEEKLY_TOP_10_REVIEWS_PATH = WEEKLY_DATA_PATH + r'reviews/'\n",
    "files = os.listdir(WEEKLY_DATA_PATH)\n",
    "try:\n",
    "    csv_file = [f for f in files if f.endswith('.txt')]\n",
    "    for file in csv_file:\n",
    "        reviews = spark.read.text(\n",
    "            WEEKLY_TOP_10_REVIEWS_PATH + file,\n",
    "        )\n",
    "        reviews.cache()\n",
    "        reviews.show()\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while reading the CSV file:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
